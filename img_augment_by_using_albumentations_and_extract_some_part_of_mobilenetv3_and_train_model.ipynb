{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    Compose, HorizontalFlip, VerticalFlip, Rotate,\n",
    "    RandomBrightnessContrast, HueSaturationValue,\n",
    "    ShiftScaleRotate, RandomResizedCrop, RandomBrightnessContrast,\n",
    "    MotionBlur, MedianBlur, GaussianBlur, GaussNoise, \n",
    "    ElasticTransform, GridDistortion, OpticalDistortion,\n",
    "    Cutout, CoarseDropout, Blur, RGBShift, \n",
    "    CLAHE, ChannelShuffle, ToGray\n",
    ")\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define the augmentation pipeline\n",
    "augmentation = Compose([\n",
    "    HorizontalFlip(p=0.5),\n",
    "#    VerticalFlip(p=0.5),\n",
    "  Rotate(limit=20, p=1),\n",
    "     RandomBrightnessContrast(p=0.3),\n",
    "#     HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=1),\n",
    "#     ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=45, p=0.5),\n",
    "#        RandomResizedCrop(height=120, width=160, scale=(0.3, 0.5), p=1),\n",
    "#     MotionBlur(blur_limit=(15, 31), p=1),\n",
    "#     MedianBlur(blur_limit=(5, 7), p=1),\n",
    "#     GaussianBlur(blur_limit=(3, 7), p=1),\n",
    "#     GaussNoise(var_limit=(100.0, 500.0), p=1),\n",
    "#    ElasticTransform(alpha=1, sigma=10, alpha_affine=10, p=1),\n",
    "#     GridDistortion(num_steps=5, distort_limit=0.3, p=1), #not imp\n",
    "#     OpticalDistortion(distort_limit=0.3, shift_limit=0.3, p=1), # not imp\n",
    "#     Cutout(num_holes=8, max_h_size=16, max_w_size=16, p=1),\n",
    "#    CoarseDropout(max_holes=8, max_height=16, max_width=16, p=1), #imp\n",
    "#     Blur(blur_limit=3, p=1), # Not imp gaussian blur will be enough\n",
    "#     RGBShift(r_shift_limit=30, g_shift_limit=30, b_shift_limit=30, p=1) # not imp\\\n",
    " #   Shear(shar_limit=0.5, p=1),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "for i in range(100):\n",
    "    img_list = []\n",
    "\n",
    "    # Load an image\n",
    "    image = cv2.imread(\"/home/aman/ai/single_img_data_training/train/none/16873233598408537.jpg\")\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n",
    "    img_list.append(image)\n",
    "\n",
    "    # Apply the augmentation\n",
    "    augmented_image = augmentation(image=image)['image']\n",
    "    img_list.append(augmented_image)\n",
    "\n",
    "    final_img = img_list[0]\n",
    "\n",
    "    for img in img_list[1:]:\n",
    "        final_img = np.concatenate((final_img, img), axis=1)    \n",
    "    plt.imshow(final_img)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D,Dense,Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "import numpy as np\n",
    "# !pip install numpy==1.20\n",
    "import matplotlib.pyplot as plt\n",
    "# !pip install autokeras\n",
    "# import autokeras as ak\n",
    "import glob\n",
    "import os, cv2\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import MobileNetV3Small\n",
    "from tensorflow.keras.applications import MobileNetV3Large\n",
    "\n",
    "from skimage.util import random_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE=3\n",
    "\n",
    "IMAGE_SIZE_WIDTH= 80\n",
    "IMAGE_SIZE_HEIGHT = 60 \n",
    "NUM_CHANNEL = 3                                   \n",
    "\n",
    "IMAGE_SHAPE = (IMAGE_SIZE_HEIGHT, IMAGE_SIZE_WIDTH) #(height, width)\n",
    "MODEL_IMAGE_SHAPE = (IMAGE_SIZE_HEIGHT, IMAGE_SIZE_WIDTH, NUM_CHANNEL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir='/home/aman/ai/single_img_data_training/train'\n",
    "val_data_dir='/home/aman/ai/single_img_data_training/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from tensorflow.keras.layers.preprocessing.image_preprocessing import HORIZONTAL\n",
    "\n",
    "train_datagen=ImageDataGenerator(   rotation_range=15,       # Rotate images randomly up to 20 degrees\n",
    "                                    width_shift_range=0.15,   # Shift the width by a fraction of the total width\n",
    "                                    height_shift_range=0.15,  # Shift the height by a fraction of the total height\n",
    "                                    zoom_range=0.15,# Randomly zoom into images by up to 20%\n",
    "                                    shear_range=0.15, \n",
    "#                                     zca_whitening= True,\n",
    "                                    brightness_range=[0.9, 1.35], \n",
    "                                 )\n",
    "\n",
    "val_datagen=ImageDataGenerator(#rescale=1./255\n",
    "                               )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_generator=train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                  target_size=IMAGE_SHAPE, # accept height and wdith order\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "#                                                   color_mode=\"grayscale\",\n",
    "                                                  class_mode='sparse',\n",
    "                                                  \n",
    "                                                 )\n",
    "\n",
    "val_generator=val_datagen.flow_from_directory(val_data_dir,\n",
    "                                              target_size=IMAGE_SHAPE,\n",
    "                                              batch_size=BATCH_SIZE,\n",
    "#                                               color_mode=\"grayscale\",\n",
    "                                              class_mode='sparse')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator=train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                  target_size=IMAGE_SHAPE, # accept height and wdith order\n",
    "                                                  batch_size=16,\n",
    "#                                                   color_mode=\"grayscale\",\n",
    "                                                  class_mode='sparse',\n",
    "                                                  \n",
    "                                                 )\n",
    "def train_gen():\n",
    "\n",
    "    \n",
    "    for train_x, train_y in train_generator:\n",
    "        for i in range(train_x.shape[0]):\n",
    "            #print(train_x[0].shape)\n",
    "            train_x[i] = augmentation(image=train_x[i])[\"image\"]\n",
    "            #pass\n",
    "        \n",
    "        #print(\"yeild\")\n",
    "        yield train_x, train_y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = list(train_generator.class_indices.keys())\n",
    "NUM_CLASSES = len(class_names)\n",
    "print(\"Class Names : \", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names_instances = {}\n",
    "max_instances = 0\n",
    "for i, folder_path in enumerate(sorted(glob.glob(train_data_dir+\"/*\"))):\n",
    "\n",
    "    class_names_instances[folder_path.split(\"/\")[-1]] = len(os.listdir(folder_path))\n",
    "    if max_instances < len(os.listdir(folder_path)):\n",
    "        max_instances = len(os.listdir(folder_path))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {}\n",
    "\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_weights[i] = max_instances/class_names_instances[class_name]\n",
    "\n",
    "\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"{i} : {class_name}\")\n",
    "    \n",
    "print(\" \")\n",
    "\n",
    "print(\"class_weights : \",class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(train_gen())\n",
    "batch_size =5\n",
    "fig, axes = plt.subplots(nrows=1, ncols=batch_size, figsize=(10, 4))\n",
    "\n",
    "for i in range(batch_size):\n",
    "    \n",
    "    # Rescale pixel values to [0, 1]\n",
    "    image = images[i] / 255.0\n",
    "    \n",
    "    axes[i].imshow(image)\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title('{}'.format(class_names[int(labels[i])]))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.applications import MobileNetV3Small\n",
    "num_classes = 3\n",
    "\n",
    "base_model = MobileNetV3Small(\n",
    "                                input_shape=(60, 80, 3),\n",
    "                                alpha=0.01,\n",
    "                                dropout_rate=0.2,\n",
    "                                minimalistic=True, # if True, aims to provide a lightweight and efficient model for image classification tasks with minimal computational resources.\n",
    "                                 \n",
    "                                include_top=False,\n",
    "                                weights=None , #'imagenet' , None\n",
    "                                \n",
    "                             )\n",
    "base_model.trainable = True\n",
    "\n",
    "print(base_model.summary())\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "#     layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "#     layers.Dense(16, activation='relu'),\n",
    "#     layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "\n",
    "# Get the number of layers in the base model\n",
    "num_layers = len(base_model.layers)\n",
    "\n",
    "# Create a new model without the last 10 layers\n",
    "new_base_model = Model(base_model.input, base_model.layers[num_layers-75-1].output)\n",
    "\n",
    "new_base_model.trainable = True\n",
    "\n",
    "# Print summary\n",
    "print(new_base_model.summary())\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    new_base_model,\n",
    "    Conv2D(3, (3, 3), strides=(2, 2), padding='valid', use_bias=True),\n",
    "#     layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "#     layers.Dense(16, activation='relu'),\n",
    "#     layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"image_classification_checkpoint.h5\",\n",
    "                             monitor='val_sparse_categorical_accuracy',\n",
    "                             mode='max',\n",
    "                             save_best_only=True,verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_accuracy',\n",
    "                          min_delta=0,\n",
    "                          patience=5,\n",
    "                          verbose=1,\n",
    "                          restore_best_weights=True)\n",
    "\n",
    "# callbacks=[checkpoint,earlystop]\n",
    "callbacks=[checkpoint]\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                   optimizer=Adam(learning_rate=0.001),\n",
    "                   metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "epochs=100\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "steps_per_epoch = len(train_generator)\n",
    "\n",
    "history = model.fit(     train_gen(),\n",
    "                        #  steps_per_epoch=num_train_samples//batch_size,\n",
    "                         epochs=epochs,\n",
    "                        #validation_split=0.15,\n",
    "                        callbacks=callbacks,\n",
    "                         validation_data=val_generator,\n",
    "                        #  validation_steps=num_val_samples//batch_size\n",
    "                        #class_weight=class_weights,\n",
    "                    steps_per_epoch=steps_per_epoch\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "\n",
    "plot_model(base_model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# Display the image in jupyter\n",
    "Image(filename='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "from PIL import Image\n",
    "\n",
    "# Generate the plot\n",
    "plot_model(base_model, to_file='model.png', show_shapes=True, show_layer_names=True, dpi=300)\n",
    "\n",
    "# Open the image using PIL\n",
    "im = Image.open('model.png')\n",
    "\n",
    "# Specify a new size\n",
    "new_size = (im.size[0]//3, im.size[1]//3)  # for example, to reduce size by half\n",
    "\n",
    "# Resize the image\n",
    "im.thumbnail(new_size, Image.ANTIALIAS)\n",
    "\n",
    "# Display the image\n",
    "im.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
